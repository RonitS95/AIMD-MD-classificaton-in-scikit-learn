{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dad358-d49a-4bf7-9f8f-14f64b96c9bb",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "The inherent assumption here is that the features are independent of each other, i.e., they have little to no correlation to one another. This might make the model application in our case a little problematic but let's explore regardless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f53e9f-8ce3-489f-869f-8d43bbcb94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_AIMD = pd.read_csv(\"Dist_AIMD.csv\") \n",
    "df_MD = pd.read_csv('Dist_MD.csv')\n",
    "df_fin = pd.concat([df_AIMD, df_MD])\n",
    "\n",
    "df_shuffle = shuffle(df_fin, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data selection \n",
    "# First we shall select the closest oxygens and later add the rest to see the effects of increasing features\n",
    "# Then we will repeat it for hydrogens\n",
    "X3 = df_shuffle[['S-O1', 'C-O1', 'N-O1']]\n",
    "X6 = df_shuffle[['S-O1', 'C-O1', 'N-O1', 'S-O2', 'C-O2', 'N-O2']]\n",
    "\n",
    "H3 = df_shuffle[['S-H1', 'C-H1', 'N-H1']]\n",
    "H6 = df_shuffle[['S-H1', 'C-H1', 'N-H1', 'S-H2', 'C-H2', 'N-H2']]\n",
    "y = df_shuffle['Class']\n",
    "\n",
    "# Splitting the data into training(80%) and test(20%) set\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y, test_size=0.20, random_state=0)\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5378f-90b1-49ca-8898-b969334269bb",
   "metadata": {},
   "source": [
    "## Naive Bayes model \n",
    "\n",
    "They are highly efficient in learning and predictions but tend to be worse at generalizing compared to more sophisticated models. There are different types of NB classifiers implemented,\n",
    "\n",
    "1. Bernoulli : Useful for binary features\n",
    "2. Multinomial : Useful for discrete features (word counts)\n",
    "3. Gaussian : Useful for continuous features\n",
    "\n",
    "The [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) will be the most appropriate in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d71ecb-1b29-44ac-b083-29da68b9222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model: 3\n",
      "Train accuracy for GaussianNB and 3 features: 0.8768768768768769\n",
      "Test accuracy for GaussianNB and 3 features: 0.8809523809523809\n",
      "\n",
      "Names of the features:  ['S-O1' 'C-O1' 'N-O1']\n",
      "Mean of the features across each class:\n",
      " [[3.02567104 3.1004413  2.72250301]\n",
      " [3.12652537 3.33810842 2.82435454]]\n",
      "Variance of the features across each class:\n",
      " [[0.01068197 0.01749986 0.0049353 ]\n",
      " [0.01326852 0.03826187 0.01102737]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# There are no parameters for us to optimize.\n",
    "nb = GaussianNB()\n",
    "\n",
    "# For the 3 feature\n",
    "nb.fit(X3_train, y3_train)\n",
    "\n",
    "print(\"Number of features in the model:\",nb.n_features_in_)\n",
    "print(\"Train accuracy for GaussianNB and 3 features:\", nb.score(X3_train, y3_train))\n",
    "print(\"Test accuracy for GaussianNB and 3 features:\",nb.score(X3_test, y3_test))\n",
    "\n",
    "print(\"\\nNames of the features: \",nb.feature_names_in_)\n",
    "print(\"Mean of the features across each class:\\n\",nb.theta_)\n",
    "print(\"Variance of the features across each class:\\n\", nb.var_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824c9712-0ffa-4162-9619-9fba225f42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model: 6\n",
      "Train accuracy for GaussianNB and 6 features: 0.918918918918919\n",
      "Test accuracy for GaussianNB and 6 features: 0.9642857142857143\n",
      "\n",
      "Names of the features:  ['S-O1' 'C-O1' 'N-O1' 'S-O2' 'C-O2' 'N-O2']\n",
      "Mean of the features across each class:\n",
      " [[3.02567104 3.1004413  2.72250301 3.17551324 3.25285733 2.8031851 ]\n",
      " [3.12652537 3.33810842 2.82435454 3.29153321 3.54878417 3.0191084 ]]\n",
      "Variance of the features across each class:\n",
      " [[0.01068197 0.01749986 0.0049353  0.01632006 0.01710282 0.00457682]\n",
      " [0.01326852 0.03826187 0.01102737 0.01857891 0.02317821 0.02460831]]\n"
     ]
    }
   ],
   "source": [
    "# For the 6 feature\n",
    "nb.fit(X6_train, y6_train)\n",
    "\n",
    "print(\"Number of features in the model:\",nb.n_features_in_)\n",
    "print(\"Train accuracy for GaussianNB and 6 features:\", nb.score(X6_train, y6_train))\n",
    "print(\"Test accuracy for GaussianNB and 6 features:\",nb.score(X6_test, y6_test))\n",
    "\n",
    "print(\"\\nNames of the features: \",nb.feature_names_in_)\n",
    "print(\"Mean of the features across each class:\\n\",nb.theta_)\n",
    "print(\"Variance of the features across each class:\\n\", nb.var_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17f2e6-e9f5-4765-86d1-575c4ad851a1",
   "metadata": {},
   "source": [
    "### Naive bayes classifiers are probabilistic classifers based on applying Bayes' theorem and the assumption that the features are independent from one another. \n",
    "\n",
    "For both 3 and 6 feature O models we observe excellent accuracy performance, better than all other models discussed so far (KNN, SVC, logistic regression, decision trees). This defies our previous assumption that this model may not perform well but this also makes sense since, \n",
    "\n",
    "Gaussian naive-bayes classifiers assume that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution, and since the data we are dealing with is bond lengths across a trajectory, it makes sense that a Gaussian distribution would fit this particular type of dataset well. \n",
    "\n",
    "### Repeating the calculations for H dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372a0221-5695-44ea-a0fe-5cf3eeef0cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model: 3\n",
      "Train accuracy for GaussianNB and 3 features: 0.8768768768768769\n",
      "Test accuracy for GaussianNB and 3 features: 0.8809523809523809\n",
      "\n",
      "Names of the features:  ['S-H1' 'C-H1' 'N-H1']\n",
      "Mean of the features across each class:\n",
      " [[2.13957634 2.27921195 1.7920138 ]\n",
      " [2.23389381 2.56780574 1.88962832]]\n",
      "Variance of the features across each class:\n",
      " [[0.01933439 0.01707981 0.00602164]\n",
      " [0.02089352 0.03091966 0.01376504]]\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(H3, y, test_size=0.20, random_state=0)\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(H6, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# GaussianNB classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# 3 feature model\n",
    "nb.fit(X3_train, y3_train)\n",
    "\n",
    "print(\"Number of features in the model:\",nb.n_features_in_)\n",
    "print(\"Train accuracy for GaussianNB and 3 features:\", nb.score(X3_train, y3_train))\n",
    "print(\"Test accuracy for GaussianNB and 3 features:\",nb.score(X3_test, y3_test))\n",
    "\n",
    "print(\"\\nNames of the features: \",nb.feature_names_in_)\n",
    "print(\"Mean of the features across each class:\\n\",nb.theta_)\n",
    "print(\"Variance of the features across each class:\\n\", nb.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4930aac9-efc9-4a1f-bd76-529d7a8cbc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model: 6\n",
      "Train accuracy for GaussianNB and 6 features: 0.9219219219219219\n",
      "Test accuracy for GaussianNB and 6 features: 0.9404761904761905\n",
      "\n",
      "Names of the features:  ['S-H1' 'C-H1' 'N-H1' 'S-H2' 'C-H2' 'N-H2']\n",
      "Mean of the features across each class:\n",
      " [[2.13957634 2.27921195 1.7920138  2.33686447 2.45493872 1.88601989]\n",
      " [2.23389381 2.56780574 1.88962832 2.46349847 2.76209765 2.11397551]]\n",
      "Variance of the features across each class:\n",
      " [[0.01933439 0.01707981 0.00602164 0.02853373 0.01737959 0.00570643]\n",
      " [0.02089352 0.03091966 0.01376504 0.04079456 0.01684757 0.03719242]]\n"
     ]
    }
   ],
   "source": [
    "# 6 feature model\n",
    "nb.fit(X6_train, y6_train)\n",
    "\n",
    "print(\"Number of features in the model:\",nb.n_features_in_)\n",
    "print(\"Train accuracy for GaussianNB and 6 features:\", nb.score(X6_train, y6_train))\n",
    "print(\"Test accuracy for GaussianNB and 6 features:\",nb.score(X6_test, y6_test))\n",
    "\n",
    "print(\"\\nNames of the features: \",nb.feature_names_in_)\n",
    "print(\"Mean of the features across each class:\\n\",nb.theta_)\n",
    "print(\"Variance of the features across each class:\\n\", nb.var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426379c1-aac2-47b4-b150-0725b8149be4",
   "metadata": {},
   "source": [
    "### The accuracy scores are largely similar to the O model, which is not a surprise. \n",
    "\n",
    "Next let us see how well this model behaves when we use all 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33814527-f876-4ff2-b5e0-5f9a05ecdffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the model: 12\n",
      "Train accuracy for GaussianNB and 12 features: 0.9219219219219219\n",
      "Test accuracy for GaussianNB and 12 features: 0.9285714285714286\n",
      "\n",
      "Names of the features:  ['S-O1' 'S-O2' 'S-H1' 'S-H2' 'C-O1' 'C-O2' 'C-H1' 'C-H2' 'N-O1' 'N-O2'\n",
      " 'N-H1' 'N-H2']\n",
      "Mean of the features across each class:\n",
      " [[3.02567104 3.17551324 2.13957634 2.33686447 3.1004413  3.25285733\n",
      "  2.27921195 2.45493872 2.72250301 2.8031851  1.7920138  1.88601989]\n",
      " [3.12652537 3.29153321 2.23389381 2.46349847 3.33810842 3.54878417\n",
      "  2.56780574 2.76209765 2.82435454 3.0191084  1.88962832 2.11397551]]\n",
      "Variance of the features across each class:\n",
      " [[0.01068197 0.01632006 0.01933439 0.02853373 0.01749986 0.01710282\n",
      "  0.01707981 0.01737959 0.0049353  0.00457682 0.00602164 0.00570643]\n",
      " [0.01326852 0.01857891 0.02089352 0.04079456 0.03826187 0.02317821\n",
      "  0.03091966 0.01684757 0.01102737 0.02460831 0.01376504 0.03719242]]\n"
     ]
    }
   ],
   "source": [
    "# Use the full dataset (12 features) for training, y remains the same\n",
    "X = df_shuffle.iloc[:,2:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Train Gaussian NB\n",
    "\n",
    "nb = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "print(\"Number of features in the model:\",nb.n_features_in_)\n",
    "print(\"Train accuracy for GaussianNB and 12 features:\", nb.score(X_train, y_train))\n",
    "print(\"Test accuracy for GaussianNB and 12 features:\",nb.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nNames of the features: \",nb.feature_names_in_)\n",
    "print(\"Mean of the features across each class:\\n\",nb.theta_)\n",
    "print(\"Variance of the features across each class:\\n\", nb.var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e253765-f9ba-47de-bc5b-d02ab2f2df2b",
   "metadata": {},
   "source": [
    "### Finally, let us take a look at the cross-validation scores for this 12 feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21ab907-b03a-46fd-a09e-f3de7af31605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold cross valildation for the GaussianNB with 12 features:\n",
      "[0.92857143 0.92857143 0.93975904 0.96385542 0.87951807] 0.9280550774526679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"5 fold cross valildation for the GaussianNB with 12 features:\")\n",
    "cv_scores = cross_val_score(GaussianNB(), X, y, cv=5)\n",
    "print(cv_scores, np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33279c48-ac12-4480-b3a3-ed052b2f66b7",
   "metadata": {},
   "source": [
    "Slight improvement but not much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7eea0-2328-4f42-9d6a-8c2f39d8b089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyproj",
   "language": "python",
   "name": "pyproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
